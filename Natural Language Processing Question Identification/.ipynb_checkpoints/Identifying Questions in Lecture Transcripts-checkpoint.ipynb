{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4212539-cb07-4baf-a3d5-08f1786a1b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import io\n",
    "import base64\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "617b9e46-8c73-4e61-a7d8-aa5b54c601db",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ad33473-4e95-411b-ba8f-22be915677ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        # Preserve important question words\n",
    "        self.question_words = {'what', 'how', 'why', 'when', 'where', 'who', 'which', 'whose', 'whom'}\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s\\?\\!\\.]', '', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_tokens = [word for word in tokens \n",
    "                          if word not in self.stop_words or word in self.question_words]\n",
    "        return ' '.join(filtered_tokens)\n",
    "    \n",
    "    def lemmatize_text(self, text):\n",
    "        tokens = word_tokenize(text)\n",
    "        lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        return ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        text = self.clean_text(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        text = self.lemmatize_text(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26da8297-8672-42ab-b5b7-9dfafeb8bca7",
   "metadata": {},
   "source": [
    "## MODEL TRAINING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c2f31a9-7603-4e91-8e04-fe17890226f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = {}\n",
    "preprocessor = TextPreprocessor()\n",
    "sample_data = None\n",
    "\n",
    "def create_sample_dataset():\n",
    "    sample_data = {\n",
    "        'text': [\n",
    "            \"What is the capital of France?\",\n",
    "            \"The capital of France is Paris.\",\n",
    "            \"How do we solve this equation?\",\n",
    "            \"Machine learning is a subset of artificial intelligence.\",\n",
    "            \"Can you explain the concept of neural networks?\",\n",
    "            \"Neural networks are inspired by biological neurons.\",\n",
    "            \"What are the main components of a computer?\",\n",
    "            \"A computer consists of hardware and software components.\",\n",
    "            \"Why is data preprocessing important?\",\n",
    "            \"Data preprocessing helps improve model performance.\",\n",
    "            \"How does gradient descent work?\",\n",
    "            \"Gradient descent is an optimization algorithm.\",\n",
    "            \"What is the difference between supervised and unsupervised learning?\",\n",
    "            \"Supervised learning uses labeled data for training.\",\n",
    "            \"When should we use cross-validation?\",\n",
    "            \"Cross-validation helps assess model generalization.\",\n",
    "            \"What is overfitting in machine learning?\",\n",
    "            \"Overfitting occurs when a model learns training data too well.\",\n",
    "            \"How can we prevent overfitting?\",\n",
    "            \"Regularization techniques can help prevent overfitting.\",\n",
    "            \"What is the purpose of feature scaling?\",\n",
    "            \"Feature scaling normalizes the range of features.\",\n",
    "            \"Why do we split data into training and testing sets?\",\n",
    "            \"Data splitting helps evaluate model performance on unseen data.\",\n",
    "            \"What is the bias-variance tradeoff?\",\n",
    "            \"The bias-variance tradeoff balances model complexity and performance.\",\n",
    "            \"Could you clarify this concept?\",\n",
    "            \"This concept is fundamental to understanding AI.\",\n",
    "            \"Is there a better approach to this problem?\",\n",
    "            \"There are multiple approaches to solve this problem.\",\n",
    "            \"Which algorithm performs better?\",\n",
    "            \"Algorithm performance depends on the specific use case.\",\n",
    "            \"How do you implement this in Python?\",\n",
    "            \"Python implementation requires several libraries.\",\n",
    "            \"What happens if we increase the learning rate?\",\n",
    "            \"Increasing the learning rate may cause convergence issues.\"\n",
    "        ],\n",
    "        'label': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "    }\n",
    "    return pd.DataFrame(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc57c77-0021-470e-b134-f2b57494ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(vectorizer_type, classifier_type, progress=gr.Progress()):\n",
    "    global trained_models, sample_data\n",
    "    \n",
    "    progress(0, desc=\"Loading data...\")\n",
    "    \n",
    "    # Get sample data\n",
    "    if sample_data is None:\n",
    "        sample_data = create_sample_dataset()\n",
    "    \n",
    "    df = sample_data.copy()\n",
    "    \n",
    "    progress(0.2, desc=\"Preprocessing text...\")\n",
    "    \n",
    "    # Preprocess text\n",
    "    df['text_processed'] = df['text'].apply(preprocessor.preprocess)\n",
    "    \n",
    "    progress(0.4, desc=\"Creating vectorizer...\")\n",
    "    \n",
    "    # Create vectorizer\n",
    "    vectorizers = {\n",
    "        'TF-IDF Unigram': TfidfVectorizer(max_features=1000, ngram_range=(1, 1)),\n",
    "        'TF-IDF Bigram': TfidfVectorizer(max_features=1000, ngram_range=(1, 2)),\n",
    "        'Bag of Words': CountVectorizer(max_features=1000, ngram_range=(1, 1)),\n",
    "        'Character N-grams': TfidfVectorizer(analyzer='char', ngram_range=(2, 4), max_features=1000)\n",
    "    }\n",
    "    \n",
    "    classifiers = {\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'SVM Linear': SVC(kernel='linear', probability=True, random_state=42),\n",
    "        'Naive Bayes': MultinomialNB()\n",
    "    }\n",
    "    \n",
    "    vectorizer = vectorizers[vectorizer_type]\n",
    "    classifier = classifiers[classifier_type]\n",
    "    \n",
    "    progress(0.6, desc=\"Extracting features...\")\n",
    "    \n",
    "    # Extract features\n",
    "    X = vectorizer.fit_transform(df['text_processed'])\n",
    "    y = df['label']\n",
    "    \n",
    "    progress(0.8, desc=\"Training model...\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    y_pred_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    cv_scores = cross_val_score(classifier, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    \n",
    "    progress(1.0, desc=\"Saving model...\")\n",
    "    \n",
    "    # Store trained model\n",
    "    model_key = f\"{vectorizer_type}_{classifier_type}\"\n",
    "    trained_models[model_key] = {\n",
    "        'vectorizer': vectorizer,\n",
    "        'classifier': classifier,\n",
    "        'roc_auc': roc_auc,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    return (\n",
    "        f\"Model trained successfully!\\n\\n\"\n",
    "        f\"Configuration: {classifier_type} with {vectorizer_type}\\n\"\n",
    "        f\"ROC AUC: {roc_auc:.4f}\\n\"\n",
    "        f\"Cross-validation: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\\n\"\n",
    "        f\"Accuracy: {report['accuracy']:.4f}\\n\"\n",
    "        f\"Precision (Questions): {report['1']['precision']:.4f}\\n\"\n",
    "        f\"Recall (Questions): {report['1']['recall']:.4f}\\n\"\n",
    "        f\"F1-Score (Questions): {report['1']['f1-score']:.4f}\",\n",
    "        model_key\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8a3ac5a-6dfd-4dce-87d4-ddf87a41389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_text(text, model_key):\n",
    "    \"\"\"Predict whether a single text is a question\"\"\"\n",
    "    if not text.strip():\n",
    "        return \"Please enter some text to analyze.\"\n",
    "    \n",
    "    if not model_key or model_key not in trained_models:\n",
    "        return \"Please train a model first.\"\n",
    "    \n",
    "    model_info = trained_models[model_key]\n",
    "    \n",
    "    # Preprocess text\n",
    "    processed_text = preprocessor.preprocess(text)\n",
    "    \n",
    "    # Vectorize\n",
    "    text_vector = model_info['vectorizer'].transform([processed_text])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model_info['classifier'].predict(text_vector)[0]\n",
    "    probabilities = model_info['classifier'].predict_proba(text_vector)[0]\n",
    "    \n",
    "    result = {\n",
    "        'Original Text': text,\n",
    "        'Processed Text': processed_text,\n",
    "        'Prediction': 'Question' if prediction == 1 else 'Statement',\n",
    "        'Confidence': f\"{max(probabilities):.1%}\",\n",
    "        'Question Probability': f\"{probabilities[1]:.1%}\",\n",
    "        'Statement Probability': f\"{probabilities[0]:.1%}\"\n",
    "    }\n",
    "    \n",
    "    # Format output\n",
    "    output = []\n",
    "    for key, value in result.items():\n",
    "        output.append(f\"**{key}:** {value}\")\n",
    "    \n",
    "    return \"\\n\".join(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b04cd63-59f9-4afd-923f-caf12f6d77cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text_batch(text_batch, model_key):\n",
    "    \"\"\"Analyze multiple sentences\"\"\"\n",
    "    if not text_batch.strip():\n",
    "        return \"Please enter text to analyze.\"\n",
    "    \n",
    "    if not model_key or model_key not in trained_models:\n",
    "        return \"Please train a model first.\"\n",
    "    \n",
    "    # Split text into sentences\n",
    "    sentences = [s.strip() for s in text_batch.split('\\n') if s.strip()]\n",
    "    \n",
    "    if not sentences:\n",
    "        return \"No valid sentences found.\"\n",
    "    \n",
    "    model_info = trained_models[model_key]\n",
    "    results = []\n",
    "    \n",
    "    for i, sentence in enumerate(sentences, 1):\n",
    "        # Preprocess\n",
    "        processed = preprocessor.preprocess(sentence)\n",
    "        \n",
    "        # Vectorize and predict\n",
    "        text_vector = model_info['vectorizer'].transform([processed])\n",
    "        prediction = model_info['classifier'].predict(text_vector)[0]\n",
    "        probabilities = model_info['classifier'].predict_proba(text_vector)[0]\n",
    "        \n",
    "        results.append({\n",
    "            'Sentence': i,\n",
    "            'Text': sentence,\n",
    "            'Prediction': 'Question' if prediction == 1 else 'Statement',\n",
    "            'Confidence': f\"{max(probabilities):.1%}\",\n",
    "            'Question_Prob': probabilities[1]\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame for display\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_sentences = len(results)\n",
    "    questions = sum(1 for r in results if r['Prediction'] == 'Question')\n",
    "    statements = total_sentences - questions\n",
    "    \n",
    "    summary = (\n",
    "        f\"**Analysis Summary:**\\n\"\n",
    "        f\"Total Sentences: {total_sentences}\\n\"\n",
    "        f\"Questions: {questions} ({questions/total_sentences:.1%})\\n\"\n",
    "        f\"Statements: {statements} ({statements/total_sentences:.1%})\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    # Format detailed results\n",
    "    detailed = \"**Detailed Results:**\\n\"\n",
    "    for result in results:\n",
    "        detailed += (\n",
    "            f\"{result['Sentence']}. {result['Text']}\\n\"\n",
    "            f\"   → {result['Prediction']} ({result['Confidence']})\\n\\n\"\n",
    "        )\n",
    "    \n",
    "    return summary + detailed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acbf7c10-89c4-4ed1-91fc-e20bdd6682d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_roc_plot(model_key):\n",
    "    \"\"\"Create ROC curve plot\"\"\"\n",
    "    if not model_key or model_key not in trained_models:\n",
    "        return None\n",
    "    \n",
    "    model_info = trained_models[model_key]\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(model_info['y_test'], model_info['y_pred_proba'])\n",
    "    roc_auc = model_info['roc_auc']\n",
    "    \n",
    "    # Create plot\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # ROC curve\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=fpr, y=tpr,\n",
    "        mode='lines',\n",
    "        name=f'ROC Curve (AUC = {roc_auc:.3f})',\n",
    "        line=dict(color='blue', width=2)\n",
    "    ))\n",
    "    \n",
    "    # Diagonal line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[0, 1], y=[0, 1],\n",
    "        mode='lines',\n",
    "        name='Random Classifier',\n",
    "        line=dict(color='red', dash='dash')\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'ROC Curve - {model_key.replace(\"_\", \" with \")}',\n",
    "        xaxis_title='False Positive Rate',\n",
    "        yaxis_title='True Positive Rate',\n",
    "        showlegend=True,\n",
    "        width=600,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17494b99-778b-408b-8f7d-f8776ffbaecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix_plot(model_key):\n",
    "    \"\"\"Create confusion matrix plot\"\"\"\n",
    "    if not model_key or model_key not in trained_models:\n",
    "        return None\n",
    "    \n",
    "    model_info = trained_models[model_key]\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(model_info['y_test'], model_info['y_pred'])\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig = px.imshow(\n",
    "        cm,\n",
    "        labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"),\n",
    "        x=['Statement', 'Question'],\n",
    "        y=['Statement', 'Question'],\n",
    "        color_continuous_scale='Blues',\n",
    "        title=f'Confusion Matrix - {model_key.replace(\"_\", \" with \")}'\n",
    "    )\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm[0])):\n",
    "            fig.add_annotation(\n",
    "                x=j, y=i,\n",
    "                text=str(cm[i][j]),\n",
    "                showarrow=False,\n",
    "                font=dict(color=\"white\" if cm[i][j] > cm.max()/2 else \"black\", size=16)\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(width=500, height=400)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6dc1b98-611f-45b8-a243-1be185e86240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_models():\n",
    "    \"\"\"Compare all trained models\"\"\"\n",
    "    if not trained_models:\n",
    "        return \"No models trained yet. Please train some models first.\"\n",
    "    \n",
    "    # Create comparison data\n",
    "    comparison_data = []\n",
    "    for model_key, model_info in trained_models.items():\n",
    "        vectorizer_type, classifier_type = model_key.split('_', 1)\n",
    "        comparison_data.append({\n",
    "            'Model': model_key.replace('_', ' + '),\n",
    "            'Vectorizer': vectorizer_type,\n",
    "            'Classifier': classifier_type,\n",
    "            'ROC_AUC': model_info['roc_auc'],\n",
    "            'CV_Mean': model_info['cv_mean'],\n",
    "            'CV_Std': model_info['cv_std']\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    df_comparison = df_comparison.sort_values('ROC_AUC', ascending=False)\n",
    "    \n",
    "    # Format for display\n",
    "    output = \"**Model Performance Comparison:**\\n\\n\"\n",
    "    for _, row in df_comparison.iterrows():\n",
    "        output += (\n",
    "            f\"**{row['Model']}**\\n\"\n",
    "            f\"ROC AUC: {row['ROC_AUC']:.4f}\\n\"\n",
    "            f\"CV Score: {row['CV_Mean']:.4f} (±{row['CV_Std']:.4f})\\n\\n\"\n",
    "        )\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c047620-db07-41bc-8891-688c45e2e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_and_analyze_file(file, model_key):\n",
    "    \"\"\"Analyze uploaded text file\"\"\"\n",
    "    if file is None:\n",
    "        return \"Please upload a text file.\"\n",
    "    \n",
    "    if not model_key or model_key not in trained_models:\n",
    "        return \"Please train a model first.\"\n",
    "    \n",
    "    try:\n",
    "        # Read file content\n",
    "        content = file.decode('utf-8') if isinstance(file, bytes) else str(file)\n",
    "        \n",
    "        # Split into sentences\n",
    "        sentences = [s.strip() for s in content.split('\\n') if s.strip()]\n",
    "        \n",
    "        if not sentences:\n",
    "            return \"No valid sentences found in the file.\"\n",
    "        \n",
    "        # Analyze with batch function\n",
    "        return analyze_text_batch('\\n'.join(sentences), model_key)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error processing file: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dc8a2a7-1f41-4e62-88af-8be07522dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gradio_interface():\n",
    "    \"\"\"Create the main Gradio interface\"\"\"\n",
    "    \n",
    "    with gr.Blocks(title=\"Question Classification System\", theme=gr.themes.Soft()) as interface:\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        # 🎓 Question Classification System\n",
    "        ### EdTech ML Pipeline for Lecture Transcript Analysis\n",
    "        \n",
    "        This system helps identify questions within lecture transcripts using various machine learning approaches.\n",
    "        \"\"\")\n",
    "        \n",
    "        # Model training section\n",
    "        with gr.Tab(\"🔧 Model Training\"):\n",
    "            gr.Markdown(\"### Train Classification Models\")\n",
    "            gr.Markdown(\"Select a vectorizer and classifier combination to train a new model.\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    vectorizer_choice = gr.Dropdown(\n",
    "                        choices=['TF-IDF Unigram', 'TF-IDF Bigram', 'Bag of Words', 'Character N-grams'],\n",
    "                        label=\"Vectorization Method\",\n",
    "                        value='TF-IDF Unigram'\n",
    "                    )\n",
    "                    classifier_choice = gr.Dropdown(\n",
    "                        choices=['Logistic Regression', 'Random Forest', 'SVM Linear', 'Naive Bayes'],\n",
    "                        label=\"Classification Algorithm\",\n",
    "                        value='Logistic Regression'\n",
    "                    )\n",
    "                    train_btn = gr.Button(\"🚀 Train Model\", variant=\"primary\")\n",
    "                \n",
    "                with gr.Column():\n",
    "                    training_output = gr.Textbox(\n",
    "                        label=\"Training Results\",\n",
    "                        lines=10,\n",
    "                        interactive=False\n",
    "                    )\n",
    "                    current_model = gr.Textbox(\n",
    "                        label=\"Current Model Key\",\n",
    "                        visible=False\n",
    "                    )\n",
    "            \n",
    "            train_btn.click(\n",
    "                fn=train_models,\n",
    "                inputs=[vectorizer_choice, classifier_choice],\n",
    "                outputs=[training_output, current_model]\n",
    "            )\n",
    "        \n",
    "        # Single prediction section\n",
    "        with gr.Tab(\"🔍 Single Text Analysis\"):\n",
    "            gr.Markdown(\"### Analyze Individual Sentences\")\n",
    "            gr.Markdown(\"Enter a sentence to determine if it's a question or statement.\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    single_text_input = gr.Textbox(\n",
    "                        label=\"Enter text to analyze\",\n",
    "                        placeholder=\"What is machine learning?\",\n",
    "                        lines=3\n",
    "                    )\n",
    "                    model_selector1 = gr.Dropdown(\n",
    "                        label=\"Select Trained Model\",\n",
    "                        choices=[],\n",
    "                        interactive=True\n",
    "                    )\n",
    "                    analyze_single_btn = gr.Button(\"🔍 Analyze Text\", variant=\"primary\")\n",
    "                \n",
    "                with gr.Column():\n",
    "                    single_analysis_output = gr.Textbox(\n",
    "                        label=\"Analysis Results\",\n",
    "                        lines=8,\n",
    "                        interactive=False\n",
    "                    )\n",
    "            \n",
    "            analyze_single_btn.click(\n",
    "                fn=predict_single_text,\n",
    "                inputs=[single_text_input, model_selector1],\n",
    "                outputs=single_analysis_output\n",
    "            )\n",
    "        \n",
    "        # Batch analysis section\n",
    "        with gr.Tab(\"📄 Batch Text Analysis\"):\n",
    "            gr.Markdown(\"### Analyze Multiple Sentences\")\n",
    "            gr.Markdown(\"Enter multiple sentences (one per line) for batch analysis.\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    batch_text_input = gr.Textbox(\n",
    "                        label=\"Enter sentences (one per line)\",\n",
    "                        placeholder=\"What is AI?\\nAI is artificial intelligence.\\nHow does it work?\",\n",
    "                        lines=10\n",
    "                    )\n",
    "                    model_selector2 = gr.Dropdown(\n",
    "                        label=\"Select Trained Model\",\n",
    "                        choices=[],\n",
    "                        interactive=True\n",
    "                    )\n",
    "                    analyze_batch_btn = gr.Button(\"📊 Analyze Batch\", variant=\"primary\")\n",
    "                \n",
    "                with gr.Column():\n",
    "                    batch_analysis_output = gr.Textbox(\n",
    "                        label=\"Batch Analysis Results\",\n",
    "                        lines=15,\n",
    "                        interactive=False\n",
    "                    )\n",
    "            \n",
    "            analyze_batch_btn.click(\n",
    "                fn=analyze_text_batch,\n",
    "                inputs=[batch_text_input, model_selector2],\n",
    "                outputs=batch_analysis_output\n",
    "            )\n",
    "        \n",
    "        # File upload section\n",
    "        with gr.Tab(\"📁 File Analysis\"):\n",
    "            gr.Markdown(\"### Upload and Analyze Text Files\")\n",
    "            gr.Markdown(\"Upload a text file containing sentences to analyze.\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    file_upload = gr.File(\n",
    "                        label=\"Upload Text File\",\n",
    "                        file_types=[\".txt\"],\n",
    "                        type=\"binary\"\n",
    "                    )\n",
    "                    model_selector3 = gr.Dropdown(\n",
    "                        label=\"Select Trained Model\",\n",
    "                        choices=[],\n",
    "                        interactive=True\n",
    "                    )\n",
    "                    analyze_file_btn = gr.Button(\"📁 Analyze File\", variant=\"primary\")\n",
    "                \n",
    "                with gr.Column():\n",
    "                    file_analysis_output = gr.Textbox(\n",
    "                        label=\"File Analysis Results\",\n",
    "                        lines=15,\n",
    "                        interactive=False\n",
    "                    )\n",
    "            \n",
    "            analyze_file_btn.click(\n",
    "                fn=upload_and_analyze_file,\n",
    "                inputs=[file_upload, model_selector3],\n",
    "                outputs=file_analysis_output\n",
    "            )\n",
    "        \n",
    "        # Visualization section\n",
    "        with gr.Tab(\"📈 Model Visualization\"):\n",
    "            gr.Markdown(\"### Model Performance Visualization\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                model_selector4 = gr.Dropdown(\n",
    "                    label=\"Select Model for Visualization\",\n",
    "                    choices=[],\n",
    "                    interactive=True\n",
    "                )\n",
    "                viz_btn = gr.Button(\"📈 Generate Plots\", variant=\"primary\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    roc_plot = gr.Plot(label=\"ROC Curve\")\n",
    "                with gr.Column():\n",
    "                    confusion_plot = gr.Plot(label=\"Confusion Matrix\")\n",
    "            \n",
    "            viz_btn.click(\n",
    "                fn=lambda model_key: (create_roc_plot(model_key), create_confusion_matrix_plot(model_key)),\n",
    "                inputs=model_selector4,\n",
    "                outputs=[roc_plot, confusion_plot]\n",
    "            )\n",
    "        \n",
    "        # Model comparison section\n",
    "        with gr.Tab(\"⚖️ Model Comparison\"):\n",
    "            gr.Markdown(\"### Compare All Trained Models\")\n",
    "            \n",
    "            compare_btn = gr.Button(\"⚖️ Compare Models\", variant=\"primary\")\n",
    "            comparison_output = gr.Textbox(\n",
    "                label=\"Model Comparison Results\",\n",
    "                lines=15,\n",
    "                interactive=False\n",
    "            )\n",
    "            \n",
    "            compare_btn.click(\n",
    "                fn=compare_all_models,\n",
    "                outputs=comparison_output\n",
    "            )\n",
    "        \n",
    "        # Update model choices when models are trained\n",
    "        def update_model_choices():\n",
    "            choices = list(trained_models.keys())\n",
    "            return (\n",
    "                gr.Dropdown.update(choices=choices),\n",
    "                gr.Dropdown.update(choices=choices),\n",
    "                gr.Dropdown.update(choices=choices),\n",
    "                gr.Dropdown.update(choices=choices)\n",
    "            )\n",
    "        \n",
    "        # Update dropdowns when training completes\n",
    "        train_btn.click(\n",
    "            fn=update_model_choices,\n",
    "            outputs=[model_selector1, model_selector2, model_selector3, model_selector4]\n",
    "        )\n",
    "        \n",
    "        # Example section\n",
    "        with gr.Tab(\"💡 Examples & Help\"):\n",
    "            gr.Markdown()\n",
    "    \n",
    "    return interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b962f507-a360-4e6f-9e56-b0e6c8586448",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create and launch the interface\n",
    "    demo = create_gradio_interface()\n",
    "    \n",
    "    # Launch with sharing enabled for external access\n",
    "    demo.launch(\n",
    "        share=True,  # Creates a public link\n",
    "        server_name=\"0.0.0.0\",  # Allow external connections\n",
    "        server_port=7860,  # Default Gradio port\n",
    "        show_api=True,  # Show API documentation\n",
    "        favicon_path=None,\n",
    "        ssl_verify=False\n",
    "    )\n",
    "\n",
    "# Alternative launch for local development only\n",
    "def launch_local():\n",
    "    \"\"\"Launch interface for local development only\"\"\"\n",
    "    demo = create_gradio_interface()\n",
    "    demo.launch(\n",
    "        share=False,\n",
    "        server_name=\"127.0.0.1\",\n",
    "        server_port=7860,\n",
    "        show_api=True\n",
    "    )\n",
    "\n",
    "# Instructions for deployment\n",
    "\"\"\"\n",
    "DEPLOYMENT INSTRUCTIONS:\n",
    "=======================\n",
    "\n",
    "1. Local Development:\n",
    "   - Run: python gradio_ui.py\n",
    "   - Access: http://localhost:7860\n",
    "\n",
    "2. Install Required Packages:\n",
    "   pip install gradio pandas numpy matplotlib seaborn plotly scikit-learn nltk\n",
    "\n",
    "3. For Production Deployment:\n",
    "   - Use launch_local() for local-only access\n",
    "   - Use demo.launch(share=True) for public sharing\n",
    "   - Consider using Gradio Spaces for cloud deployment\n",
    "\n",
    "4. Features Available:\n",
    "   - Interactive model training\n",
    "   - Real-time text classification\n",
    "   - Batch processing\n",
    "   - File upload and analysis\n",
    "   - Performance visualization\n",
    "   - Model comparison\n",
    "\n",
    "5. Customization:\n",
    "   - Modify sample_data for your specific dataset\n",
    "   - Add more vectorizers or classifiers\n",
    "   - Customize the UI theme and layout\n",
    "   - Add additional evaluation metrics\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
